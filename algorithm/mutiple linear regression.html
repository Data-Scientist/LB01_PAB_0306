<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title><strong>多元线性回归模型</strong></title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</head>

<body>
<h1><strong>多元线性回归模型</strong></h1>

<h2><strong>第一节 多元线性回归的简介</strong></h2>

<p>在实际问题中，一个变量往往受到多个变量的影响，而多元线性回归就是研究一组自变量如何影响一个因变量的方法。多元线性回归分析是一元线性回归分析的推广,是反映一种现象或事物的数量依多种现象或事物的数量的变动而相应地变动的规律并对研究现象或事物进行有效的预测与评估。</p>

<h2><strong>第二节  多元线性回归的基本原理</strong></h2>

<h3><strong>2.1多元线性回归模型及其参数估计</strong></h3>

<h4><strong>回归模型</strong></h4>

<p>多元线性回归考虑的是因变量Y与多个自变量X1,X2,…,Xn之间的线性关系<br/>
\[ Y= \beta _0+\beta _1X_1+\beta _2X_2+\cdots+\beta _mX_m+\varepsilon  \]
其中β0,β1,β2,…,βm是未知参数，X1,X2,…,Xm是m个可以精确测量并可控制的一般变量，ε是随机误差。通常我们假定<br/>
\[ E(\varepsilon )=0,Var(\varepsilon )=\sigma ^2 \]
在作显着性检验或Bayes分析等许多情况下，我们作更强的假定：
\[ \varepsilon \sim N(0,\sigma ^2) \]
为了估计回归系数β0,β1,…,βm,我们对变量进行了n次观察，得到n组观察资料(Yi,Xi1,Xi2,…Xim),i=1,…,n。一般要求n&gt;m。于是回归关系可写为
\[ 
\left\{\begin{matrix}
 Y_1= \beta _0+\beta _1X_{11}+\beta _2X_{12}+\cdots+\beta _mX_{1m}+\varepsilon_1\\ 
 Y_2= \beta _0+\beta _1X_{21}+\beta _2X_{22}+\cdots+\beta _mX_{2m}+\varepsilon_2 \\ 
\cdots \\ 
 Y_n= \beta _0+\beta _1X_{n1}+\beta _2X_{n2}+\cdots+\beta _mX_{nm}+\varepsilon_n 
\end{matrix}\right.
 \]
其中ε1,ε2,…,εn独立同分布，都满足\( E(\varepsilon )=0,Var(\varepsilon )=\sigma ^2 \)
我们要采用矩阵形式来表示
\[  Y=\begin{bmatrix}
Y_1\\ 
Y_2\\ 
\vdots \\ 
Y_n
\end{bmatrix}

,

X=\begin{bmatrix}
1 &  X_{11} & X_{12} & \cdots  & X_{1m}\\ 
1 &  X_{21}& X_{22} & \cdots  & X_{2m}\\ 
\cdots &\cdots &\cdots &\cdots &\cdots  \\ 
1 &  X_{n1}& X_{n2} & \cdots  & X_{nm}
\end{bmatrix}

,
 \beta=\begin{bmatrix}
\beta_1\\ 
\beta_2\\ 
\vdots \\ 
\beta_n
\end{bmatrix}

,
 \varepsilon=\begin{bmatrix}
\varepsilon_1\\ 
\varepsilon_2\\ 
\vdots \\ 
\varepsilon_n
\end{bmatrix}
 \]
则多元线性回归模型为\[ Y=X^T\beta +\varepsilon \]
其中n×(m+1)矩阵X称为回归设计矩阵，一般情况下我们假定X列满秩，即rk (X)=m+1。关于误差的假定与(1.2.2)对应为\[ E(\varepsilon )=0,Var(\varepsilon )=\sigma ^2I_n \]
其中In为单位阵,上面两方程合在一起称为多元线性模型。</p>

<h4><strong>参数估计</strong></h4>

<p>定义损失函数为
\[ Q=\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_{i1}-\cdots -\beta_mX_{im} )^2 \]
使Q达到最小，即为方程组的解。在多元线性回归分析中，参数的估计方法有最小二程估计（Least Square Estimate，LSE）和极大似然估计（Maximum LikeliHood Estimate，MLE）两种。
下面以最小二乘估计(Least Square Estimate,LSE)求模型的参数。损失函数等价于残差平方和S(β)为
\[ S(\beta )=(Y-X\beta ){}'(Y-X\beta )
=\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_{i1}-\cdots -\beta_mX_{im} )^2
=\begin{Vmatrix}
Y-X\beta
\end{Vmatrix}^2 \]
因为S(β)是β的二次可微函数，极值点处的各偏导数为0。采用矩阵微商记法
\[ 
\frac{\partial S(\beta ) }{\partial \beta }
=\frac{\partial  }{\partial \beta }[(Y-X\beta ){}'(Y-X\beta )]
=\frac{\partial  }{\partial \beta }[{Y}'Y-2{Y}'X\beta -{\beta }'{X}'X\beta ]
=-2{X}'Y+2{X}'X\beta =0
 \]
即
\[ ({X}'X)\beta ={X}'Y \]
它称为正规方程。若X列满秩，则 为非奇异阵，其逆矩阵存在，左乘(1.2.12)两边得β的最小二乘解
\[ \widehat{\beta }=({X}'X)^{-1}{X}'Y \]
可以验证上式确能使S(β)达最小值。（证明略）</p>

<p>下面研究 的基本统计性质，我们以定理形式叙述并证明。<br/>
<strong>定理1</strong>  (Gauss Markov)线性回归模型
\[ Y=X\beta +\varepsilon ,E(\varepsilon )=0,Var(\varepsilon )=\sigma ^2I_n \]
中回归系数β的最小二乘解
\[ \widehat{\beta }=({X}'X)^{-1}{X}'Y \]
是β的唯一最小方差线性无偏估计。(证明略)<br/>
 需要指出的是，β的LSE的最小方差性是局限在线性无偏估计类中的，如果考虑β的一切无偏估计类，LSE就不一定是方差最小者。进一步，如果在β的有偏估计中考虑，LSE就更不见得是方差最小了。<br/>
 下面我们考虑σ2的估计。与一元情况类似，我们应该用残差平方和去构造它。记
 \[ \widetilde{Y}=Y-\widehat{Y}=Y-X\widehat{\beta }=Y-X({X}'X)^{-1}{X}'Y=[I_n-X({X}'X)^{-1}{X}']Y \]
  \( \widetilde{Y} \)称为剩余向量，或残差向量。记
\[ P_x=I_n-X({X}'X)^{-1}{X}' \]
则\( \widetilde{Y}=P_xY \) 。PX称为投影阵，投影阵有一些简单性质（略）。<br/>
残差向量\( \widetilde{Y} \) 与LSE\( \widehat{\beta } \)是互不相关的，最小二乘估计的几何解释也可说明这点。（证明略）<br/>
需要指出的是，本段引入的回归模型含有常数项β0，于是设计矩阵X有m+1列，投影阵的秩为n-m-1，σ2的无偏估计为\( \widehat{\sigma }^2=S_{RS}/(n-m-1)=\widetilde{Y}{}'\widetilde{Y}/(n-m-1)={Y}'P_xY/(n-m-1) \)。如果回归模型不含常数项，或者就将X1理解为常数项而不单设常数项，也是可以的。如果X有p列，则投影阵秩为n-p，\( \widehat{\sigma }^2=S_{RS}/(n-p)=\widetilde{Y}{}'\widetilde{Y}/(n-p)={Y}'P_xY/(n-p) \) 。<br/>
下一段我们统一采用这个记法。希望读者理解m+1与p的含意。</p>

<h3><strong>2.2多元线性回归模型的假设检验</strong></h3>

<p>要对多元线性回归模型作假设检验， 一般需要事先作出误差正态的假设。在误差正态假设下，上一段关于参数估计的计算算式与定理都成立，而且β的最小二乘估计在β的所有无偏估计类中都具有最小方差 。我们以定理形式给出误差正态假设下参数估计的分布及其推导过程。  </p>

<p><strong>定理2</strong>  设有线性模型
\[ \underset{n\times 1 }{Y}=\underset{n\times p }{X}\underset{p\times 1 }{\beta }+\underset{n\times 1 }{Y},
\varepsilon \sim N_n(0,\sigma ^2I_n) \]
rkX=p,β的最小二乘解为\( \widehat{\beta }=({X}'X)^{-1}{X}'Y \),\( \sigma^2 \)的估计为\( \hat{\sigma }^2=(Y-\hat{Y}){}'(Y-\hat{Y})/(n-p) \),则 </p>

<p>*(1)\( \hat{\beta }=N_p(\beta,\sigma^2({X}'X)^{-1}) \)  </p>

<p>*(2)\( (\hat{\beta }-\beta){}'{X}'X(\hat{\beta }-\beta)/\sigma ^2\sim \chi ^2(p) \)</p>

<p>*(3)\( \hat{\beta } \)与\( \hat{\sigma }^2 \)独立  </p>

<p>*(4)\( S_{RS}/\sigma^2=(Y-\hat{Y}){}'(Y-\hat{Y})/\sigma^2=(n-p)\hat{\sigma}^2/\sigma^2\sim \chi ^2(n-p) \)</p>

<p>（证明略）  </p>

<p>在定理2的基础上，可以作出回归方程的显着性检验。此时提出的假设为
\[ H_0:\beta _1=\beta _2=\cdots =\beta _p=0 \]
如果H0被接受，则表明用模型Y=Xβ+ε来描述Y与自变量X1，…，Xm的关系不恰当。为了建立适当统计量，可进行平方和分解：
\[ S_{TR}=\sum_{i=1}^{n}(Y_i-\bar{Y})^2=\sum_{i=1}^{n}(Y_i-\hat{Y}_i)^2+\sum_{i=1}^{n}(\hat{Y}_i-\bar{Y})^2=S_{RS}+S_{ES} \]
在误差正态假定下，当H0成立时，Y1,…，Yn独立同分布于N(0，σ2)。由于SRS与SES也是相互独立，且\[ S_{RS}/\sigma^2\sim \chi ^2(n-p) ,S_{ES}\sigma^2\sim \chi ^2(p-1) \]
于是建立F统计量
\[ F=\frac{S_{ES}/(p-1)}{S_{RS}/(n-p)}\sim F(p-1,n-p) \]
对给定显着性水平α，查得临界值Fα(p-1,n-p)，当F&gt;Fα(p-1,n-p)时，拒绝H0，即否认了Y与X1，…，Xp完全不存在任何线性关系的说法。<br/>
以上是关于各个回归系数的一揽子检验方法。如果分析细致一些，考察某个自变量Xj对Y的作用显着不显着，可以作假设\[ H_0:\beta _j=0 \]进行检验。<br/>
定理2指出\( \hat{\beta } \)与\( \hat{\sigma }^2 \)相互独立,且\( \hat{\beta }=N_p(\beta,\sigma^2({X}'X)^{-1}) \)。设\( \hat{\beta } \)的第j个分量为\( \hat{\beta }_j \),β的第j个分量为βj，\( ({X}'X)^{-1} \)的对角线上第j个元素为Cjj,则\( E(\hat{\beta }_j)=\hat{\beta }_j \), \( D(\hat{\beta }_j)=C_{jj}\sigma ^2 \)。于是有,在假设H0成立时，βj=0，于是得统计量\[ F=\frac{\hat{\beta_j^2 }/C_{jj}}{S_{RS}/(n-p)} \]
及\[ t=\frac{\hat{\beta }_j}{\sqrt{C_{jj}S_{RS}/(n-p)}} \]
可用来作假设检验，判定Xj对Y的影响是否显着。<br/>
对于判定对Y无显着性影响的Xj，原则上可以剔除。但是这方面可能产生很复杂的情况，我们留待下节仔细讨论。<br/>
上面介绍了对回归系数整体检验与个别检验。有时我们需要对部分参数的线性组合作检验。(组合模型略)  </p>

<p>在一元线性回归的显着性检验中有一个相关系数检验，统计量是rXY，一元线性回归模型推导出R2统计量，它与\( r_{XY}^2 \)有完全相同的形式。R2统计量可以推广到多元。事实上，由正规方程\( {X}'X\hat{\beta }={X}'Y \)，我们有
\[ R^2=\frac{\sum_{i=1}^{n}(\hat{Y}_i-\bar{Y})^2}{\sum_{i=1}^{n}(Y_i-\bar{Y})^2}
=\frac{[\sum_{i=1}^{n}(Y_i-\bar{Y})(\hat{Y}_i-\bar{\hat{Y}}_i)]^2}{\sum_{i=1}^{n}(\hat{Y}_i-\bar{Y})^2\cdot \sum_{i=1}^{n}(Y_i-\bar{Y})^2}
=r_Y^2\hat{Y}
 \]
我们看到了R2的几何意义，它是资料Yi (i=1,…，n)与Yi的估计值之间的相关系数的平方。当回归效果特别好时，R2应该近似于1，即表示拟合值几乎与观测值Yi重合。当回归效果特别不好时，R2近似为0，表示拟合值与观测值Yi完全不相关。可见R2是回归效果一个很好的度量。一般称R为复相关系数，或全相关系数。计算统计量R2供使用者参考，方便使用者对多元回归效果的直观观察。</p>

<h3><strong>2.3多元线性回归预测与参数的区间估计</strong></h3>

<p>在通过了线性回归的显著性检验后，可以利用回归方程作预测。点预测只须将X0 = (X01，…，X0p) 代入回归方程算出\( \hat{Y}_0=X_{0}\beta \)即可。<br/>
要作出\( \hat{Y}_0 \)的区间估计，需要求得它的分布。（推导略）  </p>

<p>故\( \hat{Y}_0 \)的区间估计（显著水平α）为：
\[ X_0\beta \pm t_{(1-a)/2}(n-p)\cdot \hat{\sigma }\cdot \sqrt{{X}'_0({X}'X)^{-1}X_0} \]</p>

<h2><strong>第三节  自变量选择与逐步回归</strong></h2>

<p>上一节曾经讲到，当假设H0∶βj=0成立时，可以从回归模型剔除变量Xj。有时我们需要考虑相反的情况，往模型里添加一个新的变数。无论变量的剔除或添加，我们都要考虑两个问题：一是剔除与添加变数的准则是什么?二是如何选择好的线性回归模型。本节逐次解答这些问题。</p>

<h3><strong>3.1线性模型添加变量的影响</strong></h3>

<p>主要分析在线性模型中添加变量后，新旧模型的回归系数、残差平方和的变化关系。（过程略），结果是新模型中对原有回归模型的系数和残差平方和的基础上都进行了修正。</p>

<h3><strong>3.2自变量选择的准则</strong></h3>

<p>从回归方程中剔除一个自变量原则我们已经有了，就是做假设检验，若H0∶βj=0被接受，就剔除Xj。现在的问题是，剔除或添加自变量后，形成了许多的回归方程，这每一个方程里的自变量已不能再剔除了，现在如何判定这许多的回归方程中，哪一个比较好?
比如说，为了预测轿车的需求量，有人选人均收入、人口密度、人均拥有轿车数作回归，有人选钢产量、汽油产量、卡车数量作回归，各有各的道理。现在从数理统计的角度问，哪一个回归方程的自变量选得好?
这里介绍几个常用的准则。  </p>

<p>1.平均残差平方和\( \hat{\sigma }^2 \)  </p>

<p>我们在一元回归与多元回归都曾得到过平方和分解式：STS=SRS+SES,即
校正平方和=残差平方和+回归平方和<br/>
很明显,STS是不因自变量的选择而改变的。从直观看，当然是残差平方和越小越好，但是一般来说变量越选得多，残差平方和总会越小，而好的回归方程应该是变量不太多。为此我们用σ2的估计：平均残差平方和\( \hat{\sigma }^2 \) 作为选择方程的准则
\[ \hat{\sigma }^2=\frac{1}{n-p}\sum_{i=1}^{n}(Y_i-\hat{Y}_i)^2 \]
\( \hat{\sigma }^2 \) 越小就表示所选的回归方程越好。    </p>

<p>2.修正的全相关系数R  </p>

<p>\( R^2=1-\frac{S_{RS}}{S_{TS}} \)
由于STS是常量，R2越大意味着残差平方和越小，而自变量越多必有残差平方和越小，所以需要考虑将R加以修正。我们定义修正的全相关系数为
\[ \bar{R}_p^2=1-(1-R_p^2)\frac{n}{n-p} \]
用来度量回归拟合的好坏。这里p是自变量的个数。在(1.3.13)里p越大将导致 越小，体现了选自变量不宜太多的原则。</p>

<p>3.预测偏差的方差(n+p)\( \hat{\sigma }^2 \) </p>

<p>回归方程在n个观测点上有n个预测值，第i个预测值的偏差为<br/>
\[ D_i=Y_i-{X_i}'\hat{\beta }{}' \]
它是一个随机变量，显然我们希望它的方差越小越好。将Di (i=1,…,n)排成向量,则<br/>
\[ Var(D)=\sigma ^2+XVar(\hat{\beta }){X}'=\sigma ^2(1+X({X}'X)^{-1}{X}') \]
于是各观测点的预测偏差平方和为Var(D)对角线元素之和： 
\[ \sum_{i=1}^{n}Var(D_i)=n\sigma ^2+\sigma ^2tr(X({X}'X)^{-1}{X}')=(n+p)\sigma ^2 \]
用\( \hat{\sigma }^2 \)代替σ2，我们就得到预测偏差方差之和的一个估计：(n+p)\( \hat{\sigma }^2 \)，可以用来度量自变量选择的优劣。</p>

<p>4.Cp统计量  </p>

<p>建立回归方程的目的在于预测，L.Mallows(1964)提出的Cp统计量就是从预测角角度来衡量回归方程优劣的。考虑原回归模型
\[ \underset{n\times 1 }{Y}=\underset{n\times k }{X}\underset{k\times 1 }{\beta }+\underset{n\times 1 }{Y},
\varepsilon \sim N_n(0,\sigma ^2I_n) \]
注意这里X是k列。将βk×1分为两块
\[ \beta =\begin{bmatrix}
\beta _p\\ 
\beta _q
\end{bmatrix}
 \]
这里p+q=k。相应X也分为两块
\[ X=\begin{pmatrix}
 X_p& \vdots  & X_q
\end{pmatrix}
 \]
考虑去掉q个变量后的新模型
\[ \underset{n\times 1 }{Y}=\underset{n\times p }{X}\underset{p\times 1 }{\beta }+\underset{n\times 1 }{Y} \]
设它的最小二乘估计的残差平方和为SRSp,则所谓Cp统计量是
\[ C_p=\frac{S_{RSP}}{\hat{\sigma }^2}-n+2p \]
原则是Cp值越小新的回归方程越好。<br/>
直观上Cp统计量是可以接受的，它表示新模型的残差平方和较小。要弄清为什么Cp统计量有这个形式，还须弄清另一个含有未知参数的量Jp。(推导证明略)</p>

<h3><strong>3.3逐步回归</strong></h3>

<p>逐步回归是逐步筛选自变量的回归，筛选过程是有进有出。开始时，将因变量与每一自变量作一元回归，挑出与Y相关最密切或F检验最显著的一元线性回归方程。然后再引入第二个变量，原则是它比别的变量进入模型有更大的F检验值。同时对原来的第一个变量作检验，看新变量引入后老变量还是否显著，若不显著则予以剔除。如此继续下去，每次都引入一个在剩余变量中进入模型有最大F检验值的变量，每次引入后又对原来已引入的变量逐一检验以决定是否剔除。这样直到再无新变量可以引入，同时再无旧变量可以剔除为止，最终建立起回归方程。
进行逐步回归通常由以下步骤组成。
1.观测数据标准化  </p>

<p>这样做的目的一是提高计算精度，二是有利于数据转换，标准化按列进行。</p>

<p>2.选入变数  </p>

<p>将模型外的变量分别进入模型计算其F检验值，挑F值最大的实际进入。选入变量后各个参数的最小二乘解不必重新计算矩阵的逆，而只需按本节第一段所提供的方法对原有参数加以修正即可。</p>

<p>3.剔除变数  </p>

<p>一般采用F检验。逐次假设βj=0。取这p个F值中最小的一个，作F检验决定它是否应该剔除。<br/>
4.整理结果  </p>

<p>当再无新自变量可以进入且无旧自变量可以剔除时，终止筛选变量并计算回归方程各项参数、检验指标并作出预测。</p>

<h3><strong>3.4逐级回归</strong></h3>

<p>逐步回归是每一次选一个自变量X，与原来入选自变量一起对因变量Y再作回归。还有一种所谓逐级回归，思想有点不同。逐级回归作法如下：先选一个与Y相关性最大的X(1)，对Y回归，得到残差：
\[ \tilde{Y}^{(1)}=Y-\hat{Y}=Y-X^{(1)}\beta ^{(1)} \]
再在剩余自变量中选一个与 相关最大的 ，对 回归，得到拟合残差的残差：
\[ \tilde{Y}^{(2)}=\tilde{Y}^{(1)}-\hat{Y}^{(1)}=\tilde{Y}^{(1)}-X^{(2)}\beta ^{(2)} \]
将残差拟合值加到原来拟合值中：<br/>
\[ \hat{Y}+\hat{Y}^{(1)} \]
显然对Y有更好的拟合。这个过程可以一直进行到自变量选完为止。当然，逐级回代得到的最终方程一般不再是全体自变量的最小二乘解。 
 这种逐级回归的思想在本书第十章讲到的偏最小二乘方法中有所应用。  </p>

<p>逐步回归研究的范畴属于最优子集回归方法，最优子集回归又属于最优回归方程的选择，也可以说是回归模型的选择。这些方面的研究工作还在不断深入地进行。近一点看，岭回归，主成分回归，特征根回归，是属于这方面的内容。远一点看，模型选择都是在讲最优回归方程的选择，回归模型的选择吗?  </p>

<h2><strong>第四节 多元回归诊断（待续）</strong></h2>

<h3><strong>4.1多元线性回归拟合前提假设</strong></h3>

<p>1.正态<br/>
2.独立<br/>
3.线性<br/>
4.等方差  </p>

<h3><strong>4.2多元线性回归共线性诊断</strong></h3>

<p>1.容忍度<br/>
2.方差膨胀因子<br/>
3.特征根系统  </p>

<h3><strong>4.3异常观测</strong></h3>

<p>1.离群值识别<br/>
2.高杠杆点<br/>
3.强影响点  </p>

<h2><strong>第五节 替代回归方法（待续）</strong></h2>

<h3><strong>5.1稳健回归</strong></h3>

<p>不同观测加权回归，越偏离，离群点权重越小。  </p>

<h3><strong>5.2Lasso回归</strong></h3>

<h3><strong>5.3Ridge回归</strong></h3>

<h3><strong>5.4Elastic回归</strong></h3>

<p>后面三种回归主要针对损失函数进行调整，能有效地解决共线性的问题。</p>

</body>

</html>

